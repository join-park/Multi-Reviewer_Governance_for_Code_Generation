# Multi-Reviewer_Governance_for_Code_Generation

## Overview

This project explores whether introducing a **software delivery pipeline–inspired multi-reviewer governance structure** can improve LLM code generation performance compared to traditional self-correction baselines.

Instead of relying solely on self-refinement loops, we simulate a realistic CI/CD workflow:

* A **Code Generator** writes code.
* Multiple **Reviewers** analyze the code and propose fixes.
* The Generator revises the code based on aggregated feedback.
* A **CI-style Evaluator** executes tests to verify correctness.

The goal is to measure whether structured multi-agent review improves benchmark performance.

---

## Motivation

Modern LLM code generation pipelines often use:

* Self-correction
* Execution feedback loops
* Iterative refinement

However, real-world software delivery rarely depends on a single developer.

Instead, code is:

1. Generated by an author
2. Reviewed by multiple engineers
3. Revised based on feedback
4. Validated by CI tests

This project investigates whether mimicking this governance structure benefits LLM code generation.

---

## Final Architecture (Performance-Optimized)

```
Generator → Test → Multi-Reviewer → Revision → Test → Result
```

### Detailed Flow

1. **Generator** produces initial code.
2. **Evaluator** runs unit tests.
3. If PASS → finish.
4. If FAIL:

   * Test traceback is provided to reviewers.
5. **3 Reviewers** analyze:

   * Code
   * Prompt
   * Test failure logs
6. Reviewers propose concrete fixes.
7. Generator revises code using combined feedback.
8. Revised code is re-tested.
9. Process repeats until `max_regen` is reached.

This is an **execution-aware review pipeline**.

---

## Agents

### 1. SelfCorrectionAgent (Baseline)

Implements standard execution feedback refinement:

```
Generate → Test → Fix → Test → …
```

* Uses traceback directly.
* No reviewer governance.
* Serves as comparison baseline.

---

### 2. MultiReviewerAgent (Proposed Model)

Implements governance-style review.

#### Reviewer Roles (Fixed = 3)

| Reviewer             | Responsibility                    |
| -------------------- | --------------------------------- |
| Correctness Reviewer | Logic & algorithm validity        |
| Edge-Case Reviewer   | Boundary/counterexample analysis  |
| Debugger Reviewer    | Traceback-driven bug localization |

Reviewers receive:

* Prompt
* Generated code
* Test failure traceback

They output structured fixes:

```
STATUS: LGTM or NEEDS_CHANGES
CHANGES: bullet list of concrete fixes
```

Generator integrates all reviewer feedback into a revised solution.

---

## Model Configuration

```python
base_url = "http://localhost:11434"

llm_name = "llama3.1:8b"
code_llm_name = "qwen2.5-coder:7b"

llm_temperature = 0.0
code_temperature = 0.1

review_llm_name = "qwen2.5-coder:7b"
review_temperature = 0.5
```

### Design Rationale

| Component            | Model         | Reason                |
| -------------------- | ------------- | --------------------- |
| Generator            | qwen2.5-coder | Strong code synthesis |
| Reviewer             | qwen2.5-coder | Better bug detection  |
| Temperature (gen)    | Low           | Stability             |
| Temperature (review) | Higher        | Diverse critique      |

---

## Evaluation Benchmark

We use **HumanEval**.

### Why HumanEval?

* Widely adopted code generation benchmark
* Unit-test based evaluation
* Measures functional correctness
* Supports pass@k metrics

Each task includes:

* Prompt
* Canonical solution
* Unit tests
* Entry point function

---

## Evaluator Design

`HumanEvalEvaluator` runs code safely:

* Executes prompt + completion
* Injects candidate function
* Runs unit tests
* Uses multiprocessing sandbox
* Applies timeout protection

Return format:

```
(True, "")           → PASS
(False, traceback)  → FAIL
```

---

## Repository Structure

```
src/
 ├─ agents/
 │   ├─ base_agent.py
 │   ├─ self_correction_agent.py
 │   └─ multi_reviewer_agent.py
 │
 ├─ evaluator/
 │   └─ humaneval_evaluator.py
 │
 ├─ results/
 │   ├─ run_baseline.jsonl
 │   ├─ run_baseline2.jsonl
 │   └─ ...
 │
 └─ run.py

```

---

## Installation

### 1. Install Ollama

Linux / WSL:

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

Windows:

Install via official installer:

[https://ollama.com/download](https://ollama.com/download)

---

### 2. Pull Models

```bash
ollama pull llama3.1:8b
ollama pull qwen2.5-coder:7b
```

---

### 3. Install Python Dependencies

```bash
pip install -r requirements.txt
```

---

## Running Experiments

### Self-Correction Baseline

```python
agent = SelfCorrectionAgent(cfg)
evaluator.evaluate(agent, n_tasks=50)
```

---

### Multi-Reviewer Governance Model

```python
agent = MultiReviewerAgent(
    cfg,
    num_reviewers=3,
    max_regen=1
)

evaluator.evaluate(agent, n_tasks=50)
```

---

## Logged Outputs

Per task logs include:

* Prompt preview
* Generated code
* Reviewer comments
* Revision output
* Traceback preview
* Final PASS / FAIL
* Canonical solution comparison

Results are stored as:

```
results/run.jsonl
```

---

## Research Hypothesis

> Multi-reviewer governance with execution-aware feedback improves code generation correctness compared to self-correction alone.

We test:

* Pass@1 improvement
* Error type reduction
* Revision effectiveness

---

## Key Contributions

* CI-inspired LLM code delivery pipeline
* Execution-aware reviewer governance
* Structured multi-agent feedback integration
* Comparative HumanEval benchmarking

---

## Future Extensions

* Dynamic reviewer routing
* Vote-based merge gating
* Reviewer specialization via fine-tuning
* Cost-normalized performance comparison

---

## License

MIT License

---

## Author

Project for multi-agent code generation governance research & experimentation.
